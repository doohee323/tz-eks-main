apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  annotations:
    kubectl.kubernetes.io/last-applied-configuration: |
      {"apiVersion":"monitoring.coreos.com/v1","kind":"PrometheusRule","metadata":{"annotations":{"meta.helm.sh/release-name":"prometheus","meta.helm.sh/release-namespace":"monitoring","prometheus-operator-validated":"true"},"creationTimestamp":"2021-05-19T21:14:49Z","generation":2,"labels":{"app":"kube-prometheus-stack","app.kubernetes.io/instance":"prometheus","app.kubernetes.io/managed-by":"Helm","app.kubernetes.io/part-of":"kube-prometheus-stack","app.kubernetes.io/version":"15.4.6","chart":"kube-prometheus-stack-15.4.6","heritage":"Helm","release":"prometheus"},"name":"prometheus-kube-prometheus-alertmanager.rules","namespace":"monitoring","resourceVersion":"11852296","selfLink":"/apis/monitoring.coreos.com/v1/namespaces/monitoring/prometheusrules/prometheus-kube-prometheus-alertmanager.rules","uid":"9edbe755-087d-480f-8a6d-c3259019ab35"},"spec":{"groups":[{"name":"alertmanager.rules","rules":[{"alert":"AlertmanagerConfigInconsistent","annotations":{"message":"The configuration of the instances of the Alertmanager cluster `{{ $labels.namespace }}/{{ $labels.service }}` are out of sync.\n{{ range printf \"alertmanager_config_hash{namespace=\\\"%s\\\",service=\\\"%s\\\"}\" $labels.namespace $labels.service | query }}\nConfiguration hash for pod {{ .Labels.pod }} is \"{{ printf \"%.f\" .Value }}\"\n{{ end }}\n"},"expr":"count by(namespace,service) (count_values by(namespace,service) (\"config_hash\", alertmanager_config_hash{job=\"prometheus-kube-prometheus-alertmanager\",namespace=\"monitoring\"})) != 1","for":"5m","labels":{"severity":"critical"}},{"alert":"AlertmanagerFailedReload","annotations":{"message":"Reloading Alertmanager's configuration has failed for {{ $labels.namespace }}/{{ $labels.pod}}."},"expr":"alertmanager_config_last_reload_successful{job=\"prometheus-kube-prometheus-alertmanager\",namespace=\"monitoring\"} == 0","for":"10m","labels":{"severity":"warning"}},{"alert":"AlertmanagerMembersInconsistent","annotations":{"message":"Alertmanager has not found all other members of the cluster."},"expr":"alertmanager_cluster_members{job=\"prometheus-kube-prometheus-alertmanager\",namespace=\"monitoring\"}\n  != on (service) GROUP_LEFT()\ncount by (service) (alertmanager_cluster_members{job=\"prometheus-kube-prometheus-alertmanager\",namespace=\"monitoring\"})","for":"5m","labels":{"severity":"critical"}}]},{"name":"PodHigh.rules","rules":[{"alert":"PodHighCpuLoad","annotations":{"message":"Alertmanager has found {{ $labels.instance }} with CPU too high"},"expr":"sum(up{pod_name=~\"nginx1-.*\"}) \u003e= 1","for":"1m","labels":{"severity":"critical"}}]}]}}
    meta.helm.sh/release-name: prometheus
    meta.helm.sh/release-namespace: monitoring
    prometheus-operator-validated: "true"
  creationTimestamp: "2021-05-21T22:42:05Z"
  generation: 1
  labels:
    app: kube-prometheus-stack
    app.kubernetes.io/instance: prometheus
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/part-of: kube-prometheus-stack
    app.kubernetes.io/version: 15.4.6
    chart: kube-prometheus-stack-15.4.6
    heritage: Helm
    release: prometheus
  name: prometheus-kube-prometheus-alertmanager.rules
  resourceVersion: "12276741"
  selfLink: /apis/monitoring.coreos.com/v1/namespaces/monitoring/prometheusrules/prometheus-kube-prometheus-alertmanager.rules
  uid: 8d8dade0-9c16-4339-8072-e209d8096173
spec:
  groups:
  - name: alertmanager.rules
    rules:
    - alert: AlertmanagerConfigInconsistent
      annotations:
        message: |
          The configuration of the instances of the Alertmanager cluster `{{ $labels.namespace }}/{{ $labels.service }}` are out of sync.
          {{ range printf "alertmanager_config_hash{namespace=\"%s\",service=\"%s\"}" $labels.namespace $labels.service | query }}
          Configuration hash for pod {{ .Labels.pod }} is "{{ printf "%.f" .Value }}"
          {{ end }}
      expr: count by(namespace,service) (count_values by(namespace,service) ("config_hash",
        alertmanager_config_hash{job="prometheus-kube-prometheus-alertmanager",namespace="monitoring"}))
        != 1
      for: 5m
      labels:
        severity: critical
    - alert: AlertmanagerFailedReload
      annotations:
        message: Reloading Alertmanager's configuration has failed for {{ $labels.namespace
          }}/{{ $labels.pod}}.
      expr: alertmanager_config_last_reload_successful{job="prometheus-kube-prometheus-alertmanager",namespace="monitoring"}
        == 0
      for: 10m
      labels:
        severity: warning
    - alert: AlertmanagerMembersInconsistent
      annotations:
        message: Alertmanager has not found all other members of the cluster.
      expr: |-
        alertmanager_cluster_members{job="prometheus-kube-prometheus-alertmanager",namespace="monitoring"}
          != on (service) GROUP_LEFT()
        count by (service) (alertmanager_cluster_members{job="prometheus-kube-prometheus-alertmanager",namespace="monitoring"})
      for: 5m
      labels:
        severity: critical
#  - name: sample-app-up.rules
#    rules:
#    - alert: sample-app-up
#      annotations:
#        message: |
#          `{{ $labels.namespace }}/{{ $labels.pod }}` restared.
#      expr: count(kube_pod_status_phase{pod=~"tz-sample-app.*", phase="Failed"}) by (namespace) > 0
#      for: 1m
#      labels:
#        severity: critical
  - name: blackbox-exporter
    rules:
    - alert: ProbeFailed
      expr: probe_success == 0
      for: 5m
      labels:
        severity: error
      annotations:
        summary: "Probe failed (instance {{ $labels.instance }})"
        description: "Probe failed\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
    - alert: SlowProbe
      expr: avg_over_time(probe_duration_seconds[1m]) > 1
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "Slow probe (instance {{ $labels.instance }})"
        description: "Blackbox probe took more than 1s to complete\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
    - alert: HttpStatusCode
      expr: probe_http_status_code <= 199 OR probe_http_status_code >= 400
      for: 5m
      labels:
        severity: error
      annotations:
        summary: "HTTP Status Code (instance {{ $labels.instance }})"
        description: "HTTP status code is not 200-399\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
    - alert: SslCertificateWillExpireSoon
      expr: probe_ssl_earliest_cert_expiry - time() < 86400 * 30
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "SSL certificate will expire soon (instance {{ $labels.instance }})"
        description: "SSL certificate expires in 30 days\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
    - alert: SslCertificateHasExpired
      expr: probe_ssl_earliest_cert_expiry - time()  <= 0
      for: 5m
      labels:
        severity: error
      annotations:
        summary: "SSL certificate has expired (instance {{ $labels.instance }})"
        description: "SSL certificate has expired already\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
    - alert: HttpSlowRequests
      expr: avg_over_time(probe_http_duration_seconds[1m]) > 1
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "HTTP slow requests (instance {{ $labels.instance }})"
        description: "HTTP request took more than 1s\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
    - alert: SlowPing
      expr: avg_over_time(probe_icmp_duration_seconds[1m]) > 1
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "Slow ping (instance {{ $labels.instance }})"
        description: "Blackbox ping took more than 1s\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"